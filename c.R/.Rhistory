source(statitics.R)
source('~/steam/Script/statitics.R', encoding = 'UTF-8')
install.packages("ggplot2")
n.train = 5000
n.train/2
q()
source('~/RProject/SampleProject2.R', encoding = 'UTF-8')
source('~/RProject/SampleProject2.R', encoding = 'UTF-8')
load("/run/user/1000/gvfs/sftp:host=oblivion2,user=yipeitu/home/yipeitu/steam_helpful_review_20160701/Features/FT_50_All_0.8_0.1.Rdata")
View(df.name)
source('~/RProject/binaryclassification.R')
dotplot(resamps, metric = "ROC")
source('~/RProject/f1score.R')
source('~/steam/Script/auc3.R')
library(caret)
library(ROCR)
data(iris)
iris <- iris[iris$Species == "virginica" | iris$Species == "versicolor", ]
iris$Species <- factor(iris$Species)  # setosa should be removed from factor
samples <- sample(NROW(iris), NROW(iris) * .7)
data.train <- iris[samples, ]
data.test <- iris[-samples, ]
forest.model <- train(Species ~., data.train)
y <- data.test$Species # logical array of positive / negative cases
predictions <- predict(forest.model, data.test, type="prob") # Prediction
pred <- prediction(predictions, y);
# Recall-Precision curve
RP.perf <- performance(pred, "prec", "rec");
View(iris)
View(PimaIndiansDiabetes)
sample(paste0("Class", 9:9),    size = 50,  prob = c(.2, .8), replace = FALSE)
sample(paste0("Class", 9:9),    size = 50,  prob = c(.2, .8), replace = TRUE)
sample(paste0("Class", 9:9),    size = 50,  prob = c(0.2, 0.8), replace = TRUE)
sample(PimaIndiansDiabetes$diabetes, size=20, replace = FALSE, prob = prob = c(0.2, 0.8))
sample(PimaIndiansDiabetes$diabetes, size=20, replace = FALSE, prob = c(0.2, 0.8))
sample(PimaIndiansDiabetes$diabetes, size=20, replace = FALSE, prob = 0.8)
sample(PimaIndiansDiabetes$diabetes, size=20, replace = TRUE, prob = 0.8)
sample(PimaIndiansDiabetes, size=20, replace = TRUE, prob = 0.8)
PimaIndiansDiabetes$diabetes
sample(PimaIndiansDiabetes$diabetes, size=20)
sample(PimaIndiansDiabetes$diabetes, size=20,  prob=c(.30,.70))
sample.int(PimaIndiansDiabetes$diabetes, size=20,  prob=c(.30,.70))
sample.int(PimaIndiansDiabetes$diabetes, size=20,  prob=c(.30,.70), replace = TRUE)
sample.int(PimaIndiansDiabetes$diabetes, size=20,  prob=c(.20,.80), replace = TRUE)
7/20
13/20
sample.int(PimaIndiansDiabetes$diabetes, size=20,  prob=.80, replace = TRUE)
sample.int(PimaIndiansDiabetes$diabetes, size=20,  prob=c(.80,0.20), replace = TRUE)
3/20
sample.int(PimaIndiansDiabetes$diabetes, size=20,  prob=c(.80,0.20), replace = FALSE)
sample.int(PimaIndiansDiabetes$diabetes, size=20,  prob=c(.80,0.20), replace = TRUE)
5/20
15/20
source('~/RProject/binaryclassification.R')
plot(gbmFit)
plot(gbmFit, metric = "Kappa")
print(gbmFit)
plot(gbmFit, metric = "Kappa")
summary(gbmFit)
source('~/RProject/binaryclassification.R')
source('~/RProject/binaryclassification.R')
predictions
confusionMatrix(predictions, dataTest$diabetes, positive="pos")
confusionMatrix(data=predictions, reference=dataTest$diabetes, positive="pos", mode = "prec_recall")
confMatrx <- confusionMatrix(data=predictions, reference=dataTest$diabetes, positive="pos", mode = "prec_recall")
print(confMatrx)
source('~/RProject/binaryclassification.R')
percentage <- prop.table(table(dataTrain$diabetes)) * 100
cbind(freq=table(dataTrain$diabetes), percentage=percentage)
View(data.train)
rm(ls())
rm(list=ls())
source('~/RProject/binaryclassification.R')
View(dataTrain)
percentage <- prop.table(table(PimaIndiansDiabetes$diabetes)) * 100
cbind(freq=table(PimaIndiansDiabetes$diabetes), percentage=percentage)
View(dataTrain)
dim(dataTrain)
View(dataTest)
percentage <- prop.table(table(dataTest$diabetes)) * 100
cbind(freq=table(dataTest$diabetes), percentage=percentage)
twoClassSummary(dataTest, lev = levels(dataTest$diabetes))
remove.packages("MLmetrics")
twoClassSummary(dataTest, lev = levels(dataTest$diabetes))
postResample(pred = predictions, obs = dataTest$diabetes)
twoClassSummary(dataTest, lev = levels(dataTest$diabetes))
twoClassSummary(predictions, lev = levels(dataTest$diabetes))
levels(dataTest$diabetes)
twoClassSummary(dataTest, lev = levels(dataTest$diabetes))
levels(dataTest$diabetes)
lev = levels(dataTest$diabetes)
lev
twoClassSummary(dataTest, lev = levels(dataTest$diabetes) )
source('/run/user/1000/gvfs/sftp:host=oblivion2,user=yipeitu/home/yipeitu/steam_helpful_review_20160701/c.R/practice.R')
View(dat)
prSummary(dataTest, lev = levels(dataTest$diabetes))
library(pROC)
auc(dataTest$diabetes, predictions)
pred <- prediction(predictions, dataTest$diabetes);
library(ROCR)
data(ROCR.simple)
install.packages("MLmetrics")
install.packages("MLmetrics")
library(ROCR)
data(ROCR.simple)
pred <- prediction(ROCR.simple$predictions,ROCR.simple$labels)
pred
dim(ROCR.simple)
View(dat)
rm(ls())
rm(list=ls())
library(ROCR)
data(ROCR.simple)
pred <- prediction(ROCR.simple$predictions,ROCR.simple$labels)
ROCR.simple
ROCR.simple$predictions
ROCR.simple$labels
source('~/steam/Script/auc3.R')
library(caret)
library(ROCR)
data(iris)
iris <- iris[iris$Species == "virginica" | iris$Species == "versicolor", ]
iris$Species <- factor(iris$Species)  # setosa should be removed from factor
samples <- sample(NROW(iris), NROW(iris) * .7)
data.train <- iris[samples, ]
data.test <- iris[-samples, ]
forest.model <- train(Species ~., data.train)
y <- data.test$Species # logical array of positive / negative cases
predictions <- predict(forest.model, data.test, type="prob") # Prediction
pred <- prediction(predictions, y);
pred <- prediction(predictions, dataTest$diabetes);
source('~/RProject/binaryclassification.R')
dim(predictions)
predictions <- predict(gbmFit, dataTest)
predictions
# create a simple prediction object
library(ROCR)
data(ROCR.simple)
pred <- prediction(ROCR.simple$predictions,ROCR.simple$labels)
ROCR.simple$predictions
ROCR.simple$labels
predictions
pred_labels <- ifelse(predictions == "pos", 1, 0)
pred_labels
pred_labels <- ifelse(predictions == "pos", 1, 0)
pred <- prediction(predictions, pred_labels);
pred <- prediction(predictions, pred_labels)
is.vector(predictions)
is.vector(pred_labels)
str(predictions)
pred_vector <- as.character(predictions)
pred_vector
is.vector(pred_vector)
pred_vector <- as.character(predictions)
pred_labels <- ifelse(pred_vector == "pos", 1, 0)
pred_vector
pred_labels
is.vector(pred_vector)
is.vector(pred_labels)
pred <- prediction(predictions, pred_labels)
pred <- prediction(pred_vector, pred_labels)
pred_labels = levels(pred_vector)
pred_labels
pred_labels = levels(predictions)
pred_labels
pred_vector <- as.character(predictions)
pred_labels <- as.numeric(ifelse(pred_vector == "pos", 1, 0))
pred_vector <- as.character(predictions)
pred_labels <- ifelse(pred_vector == "pos", 1, 0)
pred_vector <- as.character(predictions)
pred_labels <- as.numeric(ifelse(pred_vector == "pos", 1, 0))
pred_vector <- as.character(predictions)
pred_labels <- as.numeric(ifelse(pred_vector == "pos", 1, 0))
pred <- prediction(pred_vector, pred_labels)
ROCR.simple$labels
ROCR.simple$predictions
is.vector(ROCR.simple$predictions)
is.vector(ROCR.simple$labels)
library(ROCR)
data(ROCR.simple)
rocr.pred <- ROCR.simple$predictions
rocr.labels <- ROCR.simple$labels
pred <- prediction(rocr.pred, rocr.labels)
pred_vector <- as.numeric(ifelse(predictions == "pos", 1, 0))
target_labels <- as.numeric(ifelse(dataTest$diabetes == "pos", 1, 0))
pred <- prediction(predictions = pred_vector, labels = target_labels)
pred_vector
target_labels
pred
# create a simple prediction object
library(ROCR)
data(ROCR.simple)
rocr.pred <- ROCR.simple$predictions
rocr.labels <- ROCR.simple$labels
pred <- prediction(rocr.pred, rocr.labels)
pred
# ROC area under the curve
pred_vector <- as.numeric(ifelse(predictions == "pos", 1, 0))
target_labels <- as.numeric(ifelse(dataTest$diabetes == "pos", 1, 0))
pred <- prediction(predictions = pred_vector, labels = target_labels)
pred
confMatrx
pred <- prediction(predictions = pred_vector, labels = target_labels)
auc.tmp <- performance(pred,"auc");
auc <- as.numeric(auc.tmp@y.values)
auc.tmp
load("/run/user/1000/gvfs/sftp:host=oblivion2,user=yipeitu/home/yipeitu/steam_helpful_review_20160701/Features/FT_50_All_0.8_1.Rdata")
View(df.name)
perf <- performance(pred,"tpr","fpr")
perf
postResample(pred=pred_vector, obs=target_labels)
postResample(pred=predictions, obs=dataTest$diabetes)
#Alternative of way of AUC
dat$obs <- dataTest$diabetes
dat <- data.frame(obs =  dataTest$diabetes,
pred = prediction)
dat <- data.frame()
dat <- data.frame()
dat$obs <- dataTest$diabetes
dat$pred <- prediction
postResample(pred=predictions, obs=dataTest$diabetes)
# Recall-Precision curve
RP.perf <- performance(pred, "prec", "rec");
plot (RP.perf);
# ROC curve
ROC.perf <- performance(pred, "tpr", "fpr");
plot (ROC.perf);
pred
precision <- posPredValue(predictions, dataTest$diabetes)
recall <- sensitivity(predictions, dataTest$diabetes)
F1 <- (2 * precision * recall) / (precision + recall)
posPredValue(predictions, dataTest$diabetes)
sensitivity(predictions, dataTest$diabetes)
(2 * precision * recall) / (precision + recall)
precision <- posPredValue(predictions, dataTest$diabetes, positive="pos", negative="neg")
precision <- posPredValue(predictions, dataTest$diabetes, positive="pos", negative="neg")
recall <- sensitivity(predictions, dataTest$diabetes, positive="pos", negative="neg")
F1 <- (2 * precision * recall) / (precision + recall)
print(predictions)
#print(predictions)
confMatrx <- confusionMatrix(data=predictions, reference=dataTest$diabetes, positive="pos", mode = "prec_recall")
print(confMatrx)
confMatrx$byClass
confMatrx$overall
source('~/RProject/binaryclassification.R')
f1score <- calcF1Scores(dataTest$diabetes, predictions)
f1score <- calcF1Scores(target_labels, pred_vector)
f1score
precision <- posPredValue(pred_vector, target_labels)
recall <- sensitivity(pred_vector, target_labels)
precision <- posPredValue(as.factor(pred_vector), as.factor(target_labels))
recall <- sensitivity(as.factor(pred_vector), as.factor(target_labels))
F1 <- (2 * precision * recall) / (precision + recall)
precision <- posPredValue(as.factor(pred_vector), as.factor(target_labels), positive = "pos")
result.compare = table(dataTest$diabetes, predictions)
## predict TRUE actual TRUE
tp = result.compare[4]
## predict TRUE actual FALSE
fp = result.compare[3]
## predict FALSE actual TRUE
fn = result.compare[2]
## predict FALSE actual FALSE
tn = result.compare[1]
precision = tp / (tp+fp)
recall = tp / (tp+fn)
f.score = (2*precision*recall)/(precision+recall)
confMatrx
confMatrx$byClass
confMatrx$byClass["Precision"]
confMatrx$byClass["Recall"]
confMatrx$byClass$Recall
p<-confMatrx$byClass["Precision"]
p
p<-confMatrx$byClass[5]
p
p<-confMatrx$byClass
p
p[1]
b<-p[1]
b
b$Sensitivity
b$Sensitivity
b["Sensitivity"]
confMatrx[1]
confMatrx[2]
confMatrx[3]
confMatrx[4]
abc <- confMatrx[4]
abc$byClass
abc$byClass["Precision"]
confMatrx
confMatrx
confMatrx$byClass
confMatrx$byClass$F1
confMatrx$byClass["F1"]
x <- confMatrx$byClass["F1"]
x
x <- confMatrx$byClass[["F1"]]
x
confMatrx$byClass[["F1"]]
confMatrx$byClass[["F1"]][1]
x<-confMatrx$byClass[["F1"]]
Precision <- confMatrx$byClass[["Precision"]]
Recall <- confMatrx$byClass[["Recall"]]
F1score = (2*Precision*Recall)/(Precision+Recall)
rm(list=ls())
# load libraries
library(caret)
library(mlbench)
library(ROCR)
# load libraries
library(caret)
library(mlbench)
library(ROCR)
# load the dataset
data(PimaIndiansDiabetes)
# create a list of 80% of the rows in the original dataset we can use for training
train_index <- createDataPartition(PimaIndiansDiabetes$diabetes, p=0.80, list=FALSE)
# select 20% of the data for validation
dataTest <- PimaIndiansDiabetes[-train_index,]
# use the remaining 80% of data to training and testing the models
dataTrain <- PimaIndiansDiabetes[train_index,]
# summarize the class distribution
percentage <- prop.table(table(dataTest$diabetes)) * 100
cbind(freq=table(dataTest$diabetes), percentage=percentage)
# prepare resampling method
fitControl = trainControl(method="repeatedcv", # Baowaly changed here, it was boot
number=10,
repeats=5,
returnData = TRUE,
returnResamp = "final",
selectionFunction = "best",
summaryFunction = twoClassSummary,
classProbs = TRUE,
allowParallel = TRUE)
set.seed(7)
#Train the model
gbmFit <- train(diabetes~., data=dataTrain, method="gbm", metric="ROC", trControl=fitControl, verbose = FALSE)
print(gbmFit)
# Predic Test Dataset
test.pred <- predict(gbmFit, dataTest)
test.pred
test.ref <- dataTest$diabetes
test.ref
test.ref <- dataTest$diabetes
#print(test.pred)
test.confMatrx <- confusionMatrix(data=test.pred, reference=test.ref, positive="pos", mode = "prec_recall")
#print(confMatrx)
test.precision <- test.confMatrx$byClass[["Precision"]]
test.recall <- test.confMatrx$byClass[["Recall"]]
test.f1score <- test.confMatrx$byClass[["F1"]]
# AUC
pred_vector <- as.numeric(ifelse(test.pred == "pos", 1, 0)) #make numeric
ref_vector <- as.numeric(ifelse(test.ref == "pos", 1, 0)) #make numeric
auc.pred <- prediction(predictions = pred_vector, labels = ref_vector)
auc.tmp <- performance(auc.pred,"auc");
test.auc <- as.numeric(auc.tmp@y.values)
test.confMatrx
test.confMatrx$overall
test.confMatrx$overall[["Accuracy"]]
test.accuracy <- test.confMatrx$overall[["Accuracy"]]
test.confMatrx$byClass
test.sensitivity <- test.confMatrx$byClass[["Sensitivity"]]
test.specificity <- test.confMatrx$byClass[["Specificity"]]
test.f1score <- test.confMatrx$byClass[["F1"]]
# AUC
pred_vector <- as.numeric(ifelse(test.pred == "pos", 1, 0)) #make numeric
ref_vector <- as.numeric(ifelse(test.ref == "pos", 1, 0)) #make numeric
auc.pred <- prediction(predictions = pred_vector, labels = ref_vector)
auc.tmp <- performance(auc.pred,"auc");
test.auc <- as.numeric(auc.tmp@y.values)
#Evaluation on Training Data
train.pred <- predict(gbmFit, dataTrain)
train.ref <- dataTrain$diabetes
train.confMatrx <- confusionMatrix(data=train.pred, reference=train.ref, positive="pos", mode = "prec_recall")
train.confMatrx
train.pred <- predict(gbmFit, dataTrain)
train.ref <- dataTrain$diabetes
train.confMatrx <- confusionMatrix(data=train.pred, reference=train.ref, positive="pos", mode = "prec_recall")
train.accuracy <- train.confMatrx$overall[["Accuracy"]]
train.f1score <- train.confMatrx$byClass[["F1"]]
#Test AUC
pred_vector <- as.numeric(ifelse(train.pred == "pos", 1, 0)) #make numeric
ref_vector <- as.numeric(ifelse(train.ref == "pos", 1, 0)) #make numeric
auc.pred <- prediction(predictions = pred_vector, labels = ref_vector)
auc.tmp <- performance(auc.pred,"auc");
train.auc <- as.numeric(auc.tmp@y.values)
pred_vector
ref_vector
source('~/RProject/binaryclassification.R')
source('~/RProject/binaryclassification.R')
source('~/RProject/binaryclassification.R')
#classProbs = TRUE,
source('~/RProject/binaryclassification.R')
source('~/RProject/binaryclassification.R')
source('~/RProject/binaryclassification.R')
source('~/RProject/binaryclassification.R')
source('~/RProject/binaryclassification.R')
source('~/RProject/binaryclassification.R')
source('~/RProject/binaryclassification.R')
gbmScore
source('~/RProject/binaryclassification.R')
source('~/RProject/binaryclassification.R')
source('~/RProject/binaryclassification.R')
gbmScore
source('~/RProject/binaryclassification.R')
gbmScore
source('~/RProject/binaryclassification.R')
gbmScore
source('~/RProject/binaryclassification.R')
gbmScore
source('~/RProject/binaryclassification.R')
gbmScore
source('~/RProject/binaryclassification.R')
gbmScore
source('~/RProject/binaryclassification.R')
gbmScore
source('~/RProject/binaryclassification.R')
gbmScore
source('~/RProject/binaryclassification.R')
gbmScore
source('~/RProject/binaryclassification.R')
gbmScore
source('~/RProject/binaryclassification.R')
gbmScore
source('~/RProject/binaryclassification.R')
gbmScore
source('~/RProject/binaryclassification.R')
gbmScore
source('~/RProject/binaryclassification.R')
gbmScore
load("/run/user/1000/gvfs/sftp:host=oblivion2,user=yipeitu/home/yipeitu/steam_helpful_review_20160701/Features/FT_50_All_0.8_5.Rdata")
View(df.name)
load("/run/user/1000/gvfs/sftp:host=di2.vcloud,user=baowaly/home/baowaly/steam/Features/FT_50_All_0.8_10.Rdata")
View(df.name)
load("/run/user/1000/gvfs/sftp:host=oblivion2,user=yipeitu/home/yipeitu/steam_helpful_review_20160701/Evaluation/ES_All_50_0.8_5.Rdata")
View(comb.score)
View(comb.score)
View(comb.score)
load("/run/user/1000/gvfs/sftp:host=oblivion2,user=yipeitu/home/yipeitu/steam_helpful_review_20160701/Features/FT_50_All_0.8_10.Rdata")
View(df.name)
load("/run/user/1000/gvfs/sftp:host=oblivion2,user=yipeitu/home/yipeitu/steam_helpful_review_20160701/Features/FT_50_All_0.8_0.1.Rdata")
View(df.name)
load("/run/user/1000/gvfs/sftp:host=oblivion2,user=yipeitu/home/yipeitu/steam_helpful_review_20160701/Features/FT_50_All_0.8_0.1.Rdata")
View(feature.rank)
load("/run/user/1000/gvfs/sftp:host=oblivion2,user=yipeitu/home/yipeitu/steam_helpful_review_20160701/Features/FT_50_All_0.8_0.1.Rdata")
View(feature.rank)
load("/run/user/1000/gvfs/sftp:host=oblivion2,user=yipeitu/home/yipeitu/steam_helpful_review_20160701/Features/FT_50_All_0.8_0.1.Rdata")
View(feature.rank)
load("/run/user/1000/gvfs/sftp:host=oblivion2,user=yipeitu/home/yipeitu/steam_helpful_review_20160701/Features/FT_50_All_0.8_10.Rdata")
rm(list=ls())
load("/run/user/1000/gvfs/sftp:host=oblivion2,user=yipeitu/home/yipeitu/steam_helpful_review_20160701/Features/FT_50_All_0.8_10.Rdata")
View(feature.rank)
